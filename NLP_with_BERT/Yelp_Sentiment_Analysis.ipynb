{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NLP for Sentiment Analysis Using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use **Hugging Face Transformers** and pre-trained **BERT** Neural Networks for sentiment analysis. We will run the model using a single prompt but also leverage **BeautifulSoup** to scrape reviews from Yelp to be able to calculate sentiment on a larger scale.\n",
    "\n",
    "There are three main steps that we are going to follow:\n",
    "1. Download and install BERT from HF Transformers\n",
    "2. Run sentiment analysis Using BERT and Python\n",
    "3. Scrape reviews from Yelp and calculate the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key dependencies that we are going to need is **PyTorch**. You get PyTorch by going to https://pytorch.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
      "Requirement already satisfied: torch==1.10.1+cu113 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (1.10.1+cu113)\n",
      "Requirement already satisfied: torchvision==0.11.2+cu113 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (0.11.2+cu113)\n",
      "Requirement already satisfied: torchaudio===0.10.1+cu113 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (0.10.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from torch==1.10.1+cu113) (4.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from torchvision==0.11.2+cu113) (1.17.5)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from torchvision==0.11.2+cu113) (6.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to install 5 other dependencies; **transformers**, **requests**, **beautifulsoup4**, **pandas**, and **numpy**.\n",
    "\n",
    "We are going to leverage **transformes** for our actual NLP model. So, this is going to allows us to easily import and download and install our NLP model and specifically, the NLP model that we are going to use, the multilingual BERT model that allows us to perfomre sentiment analysis (https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment).\n",
    "This model gives you a sentiment score between 1 and 5; this means rather than just getting a confidence interval, or a number between 0 and 1, you are actually getting a score.\n",
    "\n",
    "**requests** library is going to allow us to make a request to the Yelp site that we are goint to be scraping.\n",
    "\n",
    "**beautifulsoup4** is going to allow us to actually work through that soup that we get back from the page, and extract the data we need.\n",
    "\n",
    "**Pandas** is going to allow us to structure our data in a format that makes is easy to work with. And **Numpy** is going to give us some additional data transformation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (4.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (4.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: numpy==1.17.5 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (1.17.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from transformers) (0.23)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from transformers) (5.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from requests) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from beautifulsoup4) (1.9.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.2)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (0.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: six in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\faezeh\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->transformers) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers requests beautifulsoup4 pandas numpy==1.17.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification  \n",
    "import torch  \n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# AutoTokenizer allows us to pass through a string and convert that into a sequence of numbers that we can then pass to our nlp model\n",
    "# AutoModelForSequenceClassification is going to give us the architecture from transformers to be able to load in our nlp model \n",
    "# we are going to use the arg_max function from torch to be able to extract our highest sequence result.\n",
    "# requests used to grab data or grab webpage from Yelp.\n",
    "# BeautifulSoup allows us to traverse the result from yelp, allows to extract data we actually need, the reviews.\n",
    "# allows us to creat a regex function to be able to extract the specific comments that we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instantiate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to instantiate and set up our model. First, we creat our tokenizer and then we are loading in our model. We use a pre-trained nlp model for sentiment analysis from HF (bert-base-multilingual-uncased-sentiment). There are a number of NLP models available from HF including models for translation, q&a, classification, and generation. Here we are using the model for sentiment analysis. And we set up our model using the same pre-trained model from HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fd9a3790c3469b88d63ed6da79956e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214e1a366ae842e895729bbf189afef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfc7188a50e4114b3ef41a468064971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/851k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d15eaabd57480695eb5c7de38b8f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48138fa6075f44cd9139d130ce017033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/638M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encode and Calculate Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to test our model. We are going to pass a string or a prompt to our tokenizer, tokenize it and pass it through our model and get our classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(\"I didn't like it, not recommending\", return_tensors='pt')\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to decode the tokens, but this is how it works if we want to decode it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(tokens[0])  # you should pass one list from the list of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we need to do is to pass out the tokens to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0376,  3.0210,  1.0249, -2.3583, -3.8397]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that what we are going to get out of here is a Sequence Classifier Output calss. What we need from the result to understant the sentiment is the **logits**. The values in this tensor represent the probability of that particular class being the sentiment. **More clearly, the output from the model is a one-hot encoded list of scores. The position with the highest score represents the sentiment rating**. So, in the current case, the position of the highest score is the first position, so the **rating is 1**, meaning it it is a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0376,  3.0210,  1.0249, -2.3583, -3.8397]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the rating\n",
    "int(torch.argmax(result.logits)) + 1  # since the position numbering strats from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collect Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to collect some reviews from Yelp. I am going to look at reviews for Rumi's Kitchen (one of my favorite restaurants). We are going to extract the reviews from this page https://www.yelp.com/biz/rumis-kitchen-atlanta-2 and pass them through our sentiment pipeline.\n",
    "\n",
    "To build our scraper, first we are going to use the `requests` library to grab our webpage. What we get from that is a response code, then we can type `r.text` to get the text out of that webpage, this represents everything that comprises that webpage. Then we can use `BeautifulSoup` to parse the text in this webpage. \n",
    "\n",
    "And after that using `re.compile` we are going to extract the specific components that we want from this webpage, the reviews, which are the texts that start with **comment** class (if you inspect the webpage you can see it). Then, we are going to pass out that regex through our soup, `soup.find_all`, to find all the tags within that soup that match our specific formatting. In this case, we are looking fro paragraphs, and we are looking for anything that has a class which matches our regex, which in this case is going to be **comment**.\n",
    "\n",
    "So far, we can see that the results of our code are wrapped inside of html tags, but we just want the texts. So, in the last step, we use a list comprehension to extract all the reviews from the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.yelp.com/biz/rumis-kitchen-atlanta-2')\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "regex = re.compile('.*comment.*')\n",
    "results = soup.find_all('p', {'class': regex})\n",
    "reviews = [result.text for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For starters, we got the hummus which was super yummy. It comes along with fresh pita bread \\xa0that is literally fresh out the oven. The Sabazi plate is to cleanse your plate to taste every fresh ingredients. I'm definitely a fan of the homemade sodas. The flavors they have is \\xa0peach, passion fruit and mango. The peach is my favorite thus far so definitely try it. For entrees, my boyfriend got the Lamb Koobideh Kabob and it comes with rice. I got the Chicken Kabob and it was so delicious but I definitely enjoyed the lamb more. We were so stuff that I didn't even get the chance to try a dessert. But, I'll definitely be back to Rumi Kitchen.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loade Reviews into Dataframe and score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we are going to load the reviews into a dataframe, and we are going to run through each one of these reviews and score them. Dataframe makes it easier to go through the reviews and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array(reviews), columns=['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>For starters, we got the hummus which was supe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>For starters, we got the hummus which was supe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The food was good but there were a few things ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Thai place was pretty good although my compani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>This place is fantastic!!! I've passed Rumi's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  For starters, we got the hummus which was supe...\n",
       "1  For starters, we got the hummus which was supe...\n",
       "2  The food was good but there were a few things ...\n",
       "3  Thai place was pretty good although my compani...\n",
       "4  This place is fantastic!!! I've passed Rumi's ..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For starters, we got the hummus which was super yummy. It comes along with fresh pita bread \\xa0that is literally fresh out the oven. The Sabazi plate is to cleanse your plate to taste every fresh ingredients. I'm definitely a fan of the homemade sodas. The flavors they have is \\xa0peach, passion fruit and mango. The peach is my favorite thus far so definitely try it. For entrees, my boyfriend got the Lamb Koobideh Kabob and it comes with rice. I got the Chicken Kabob and it was so delicious but I definitely enjoyed the lamb more. We were so stuff that I didn't even get the chance to try a dessert. But, I'll definitely be back to Rumi Kitchen.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviews'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to loops through each one of these reviews and get the score for them. Before that, we are going to define a quick function to do the encoding and sentiment scoring. Encapsulation the sentiment pipeline in a function makes it easier to process multiple strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(review):\n",
    "    tokens = tokenizer.encode(review, return_tensors='pt')\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits)) + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "sentiment_score(df['reviews'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to go through all the reviews and stor them inside our dataframe. To do so, we will use a `apply` `lambda` function to be able to go through, run through each one of reviews in our dataframe and store that inside of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['reviews'].apply(lambda x: sentiment_score(x[:512])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>For starters, we got the hummus which was supe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>For starters, we got the hummus which was supe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The food was good but there were a few things ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Thai place was pretty good although my compani...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>This place is fantastic!!! I've passed Rumi's ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>This place is really quite great. Food: falafe...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Excellent food, great wine, great service. Don...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Rumi's is a solid place in ATL for Persian foo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Beautifully designed restaurant with Persian a...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Rumi's is one of the best Persian spots in Atl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>the food here is great! we got a variety of di...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reviews  sentiment\n",
       "0   For starters, we got the hummus which was supe...          4\n",
       "1   For starters, we got the hummus which was supe...          4\n",
       "2   The food was good but there were a few things ...          2\n",
       "3   Thai place was pretty good although my compani...          3\n",
       "4   This place is fantastic!!! I've passed Rumi's ...          5\n",
       "5   This place is really quite great. Food: falafe...          3\n",
       "6   Excellent food, great wine, great service. Don...          5\n",
       "7   Rumi's is a solid place in ATL for Persian foo...          4\n",
       "8   Beautifully designed restaurant with Persian a...          5\n",
       "9   Rumi's is one of the best Persian spots in Atl...          4\n",
       "10  the food here is great! we got a variety of di...          5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, our NLP pipeline is actually limited as to how much text or how many tokens you can pass through to it at one particular time, and in this case, it is limited to 512 tokens. So, what we are doing here is that we grab the first 512 tokens from each reviews. This may influence the result of your sentiment pipeline, you could actually append these together or do it in multiple steps and get taken the avrage, but in this case this is a quick workaround. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this process for another restaurant or any other businesses to get the sentiment score. We just have to go to step 4 and follow all the steps from there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
